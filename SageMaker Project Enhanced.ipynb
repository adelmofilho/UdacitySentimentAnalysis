{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'update_dict' from 'enhanced.processing' (/home/aadelmo/projetos/udacity/UdacitySentimentAnalysis/enhanced/processing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-b7c1c2c3b68d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menhanced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menhanced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreparation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_imdb_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_imdb_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_train_validation_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menhanced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreview_to_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'update_dict' from 'enhanced.processing' (/home/aadelmo/projetos/udacity/UdacitySentimentAnalysis/enhanced/processing.py)"
     ]
    }
   ],
   "source": [
    "from enhanced.utils import download_data\n",
    "from enhanced.preparation import read_imdb_data, prepare_imdb_data, split_train_validation_data\n",
    "from enhanced.processing import review_to_words, preprocess_data, build_dict, update_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data parameters\n",
    "data_dir = \"data\"\n",
    "raw_folder = \"aclImdb\"\n",
    "data_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "filename = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "vocab_size = 5000\n",
    "sample_idx = 42\n",
    "cache_dir = os.path.join(\"cache\", \"sentiment_analysis\")\n",
    "pytorch_dir = 'data/pytorch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already downloaded\n"
     ]
    }
   ],
   "source": [
    "download_data(data_dir, data_url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Processing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = read_imdb_data(data_dir, raw_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split train-validation-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_X, test_X, train_valid_y, test_y = prepare_imdb_data(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, valid_X, train_y, valid_y = split_train_validation_data(train_valid_X, train_valid_y, valid_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 17500, valid = 7500, test = 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"IMDb reviews (combined): train = {}, valid = {}, test = {}\".format(len(train_X), len(valid_X),len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment: 0\n",
      "review: I went to see it in hopes of some good old fashioned Alice Entertainment.Once I realized I would not be getting that,I watched it for a pretty well made movie (in terms of filming,and yeah..that was it).But aside from it having a good film quality,considering I had been watching grainy movies all day long,there was nothing good about that movie.<br /><br />He killed 42.Why were Tweedle Dee and Dum played by Mudler and Scully?Serisouly,Who can answer that for me?Who can answer anything awful about this movie for me.<br /><br />I agree with whoever said it was just one big long inside joke for the staff.That's all it seemed to be.<br /><br />Poor Mr.Carroll.I'm so sorry somebody did that to his wonderful tales.\n"
     ]
    }
   ],
   "source": [
    "print(f\"sentiment: {train_y[sample_idx]}\")\n",
    "print(f\"review: {train_X[sample_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment: 0\n",
      "review: Friday the 13th step over! There is officially a worse movie than your hateful series out there. I won this movie in a contest at college, and it was a waste of money even if it was free. Jack Jones stars as a truly awful singer whose trying to find some murderers or something. At least Friday the Thirteenth never bored me. I'd rather have my fingernails pulled than see this again.\n"
     ]
    }
   ],
   "source": [
    "print(f\"sentiment: {valid_y[sample_idx]}\")\n",
    "print(f\"review: {valid_X[sample_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment: 1\n",
      "review: By many accounts, Stu Ungar was not a very nice guy. He spat on dealers, stiffed people he owed money to, and was verbally abusive. <br /><br />Many filmmakers might choose to sugarcoat the man, making him into some sports hero that would triumph despite adversity. But High Roller doesn't do that. And that's a tough row to hoe.<br /><br />Instead, we have to look VERY closely to see a man that never matured passed the frightened little boy from the streets of New York, despite all his successes. And the only real approval he ever gets is from death himself. Very brave (because people won't get it) and very touching (when you do).<br /><br />What is also brave is the use of a Scorsese feel. \"Aha! How derivative,\" people will say. Really? But there's virtually no violence. And Stuey LOVED gangster movies. Maybe the feel reflects the man Stu and not the director Marty? And if it really is a low budget film and looks that good, bravo!<br /><br />Finally, the linear flashback structure. Wow, will that get hammered. Yet, not only does it work, it works exceptionally well, even for those who don't see the connection to the \"Seventh Seal.\" (PROOF: In SS, Knight plays game of chess with death: In HR, Stuey says \"We can play a hand of cards for, ya know\"... Death says \"Never much good at cards..\" Damn great last line.)<br /><br />No tricky effects or camera moves. No shaky camera. Nothing trendy at all. Just solid, tight storytelling.<br /><br />Maybe that makes the movie too basic and somehow flawed. But then again, so was the guy. And that makes it just about right.<br /><br />9/10\n"
     ]
    }
   ],
   "source": [
    "print(f\"sentiment: {test_y[sample_idx]}\")\n",
    "print(f\"review: {test_X[sample_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "train_pp_X, test_pp_X, valid_pp_X, train_pp_y, test_pp_y, valid_pp_y = preprocess_data(train_X, test_X, valid_X,\n",
    "                                                                           train_y, test_y, valid_y, cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['went', 'see', 'hope', 'good', 'old', 'fashion', 'alic', 'entertain', 'realiz', 'would', 'get', 'watch', 'pretti', 'well', 'made', 'movi', 'term', 'film', 'yeah', 'asid', 'good', 'film', 'qualiti', 'consid', 'watch', 'graini', 'movi', 'day', 'long', 'noth', 'good', 'movi', 'kill', '42', 'tweedl', 'dee', 'dum', 'play', 'mudler', 'sculli', 'serisouli', 'answer', 'answer', 'anyth', 'aw', 'movi', 'agre', 'whoever', 'said', 'one', 'big', 'long', 'insid', 'joke', 'staff', 'seem', 'poor', 'mr', 'carrol', 'sorri', 'somebodi', 'wonder', 'tale']\n"
     ]
    }
   ],
   "source": [
    "print(review_to_words(train_X[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['friday', '13th', 'step', 'offici', 'wors', 'movi', 'hate', 'seri', 'movi', 'contest', 'colleg', 'wast', 'money', 'even', 'free', 'jack', 'jone', 'star', 'truli', 'aw', 'singer', 'whose', 'tri', 'find', 'murder', 'someth', 'least', 'friday', 'thirteenth', 'never', 'bore', 'rather', 'fingernail', 'pull', 'see']\n"
     ]
    }
   ],
   "source": [
    "print(review_to_words(valid_X[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mani', 'account', 'stu', 'ungar', 'nice', 'guy', 'spat', 'dealer', 'stif', 'peopl', 'owe', 'money', 'verbal', 'abus', 'mani', 'filmmak', 'might', 'choos', 'sugarcoat', 'man', 'make', 'sport', 'hero', 'would', 'triumph', 'despit', 'advers', 'high', 'roller', 'tough', 'row', 'hoe', 'instead', 'look', 'close', 'see', 'man', 'never', 'matur', 'pass', 'frighten', 'littl', 'boy', 'street', 'new', 'york', 'despit', 'success', 'real', 'approv', 'ever', 'get', 'death', 'brave', 'peopl', 'get', 'touch', 'also', 'brave', 'use', 'scorses', 'feel', 'aha', 'deriv', 'peopl', 'say', 'realli', 'virtual', 'violenc', 'stuey', 'love', 'gangster', 'movi', 'mayb', 'feel', 'reflect', 'man', 'stu', 'director', 'marti', 'realli', 'low', 'budget', 'film', 'look', 'good', 'bravo', 'final', 'linear', 'flashback', 'structur', 'wow', 'get', 'hammer', 'yet', 'work', 'work', 'except', 'well', 'even', 'see', 'connect', 'seventh', 'seal', 'proof', 'ss', 'knight', 'play', 'game', 'chess', 'death', 'hr', 'stuey', 'say', 'play', 'hand', 'card', 'ya', 'know', 'death', 'say', 'never', 'much', 'good', 'card', 'damn', 'great', 'last', 'line', 'tricki', 'effect', 'camera', 'move', 'shaki', 'camera', 'noth', 'trendi', 'solid', 'tight', 'storytel', 'mayb', 'make', 'movi', 'basic', 'somehow', 'flaw', 'guy', 'make', 'right', '9', '10']\n"
     ]
    }
   ],
   "source": [
    "print(review_to_words(test_X[sample_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Build dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict, sorted_words, word_count = build_dict(train_pp_X, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26298</th>\n",
       "      <td>movi</td>\n",
       "      <td>36020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14025</th>\n",
       "      <td>film</td>\n",
       "      <td>33312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28101</th>\n",
       "      <td>one</td>\n",
       "      <td>19170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22800</th>\n",
       "      <td>like</td>\n",
       "      <td>16026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39527</th>\n",
       "      <td>time</td>\n",
       "      <td>11449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  count\n",
       "26298  movi  36020\n",
       "14025  film  33312\n",
       "28101   one  19170\n",
       "22800  like  16026\n",
       "39527  time  11449"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_count.items(), columns = [\"word\", \"count\"]).sort_values(by=\"count\",ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = [\"movi\", \"film\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dict, new_vocab_size = update_dict(word_dict, words_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(pytorch_dir): # Make sure that the folder exists\n",
    "    os.makedirs(pytorch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pytorch_dir, 'word_dict_enhanced.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_X, train_X_tf_len = convert_and_pad_data(word_dict, train_pp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tf_X, test_X_tf_len = convert_and_pad_data(word_dict, test_pp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tf_X, valid_X_tf_len = convert_and_pad_data(word_dict, valid_pp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_tf_len), pd.DataFrame(train_tf_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(pytorch_dir, 'train.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([pd.DataFrame(valid_y), pd.DataFrame(valid_X_tf_len), pd.DataFrame(valid_tf_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(pytorch_dir, 'valid.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.session.Session(region_name=\"us-east-1\"))\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sentiment_rnn'\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except Exception as err:\n",
    "    role = \"arn:aws:iam::977053370764:role/service-role/AmazonSageMaker-ExecutionRole-20201202T141643\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=pytorch_dir, bucket=bucket, key_prefix=prefix)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Read in only the first 250 rows\n",
    "train_sample = pd.read_csv(os.path.join(pytorch_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
    "valid_sample = pd.read_csv(os.path.join(pytorch_dir, 'valid.csv'), header=None, names=None, nrows=250)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "valid_sample_y = torch.from_numpy(valid_sample[[0]].values).float().squeeze()\n",
    "valid_sample_X = torch.from_numpy(valid_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
    "valid_sample_ds = torch.utils.data.TensorDataset(valid_sample_X, valid_sample_y)\n",
    "\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)\n",
    "valid_sample_dl = torch.utils.data.DataLoader(valid_sample_ds, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "def train(model, train_loader, valid_loader, epochs, optimizer, loss_fn, device):\n",
    "    report_train = pd.DataFrame(columns=[\"epoch\", \"trainError\", \"validError\"])\n",
    "    best_valid_BCELoss = 9999999999\n",
    "    BCELoss_list = []\n",
    "    valid_BCELoss_list = []\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_valid_loss = 0        \n",
    "        for batch in train_loader:         \n",
    "            batch_X, batch_y = batch\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # TODO: Complete this train method to train the model provided.\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = loss_fn(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()            \n",
    "            total_loss += loss.data.item()\n",
    "        for block in valid_loader:     \n",
    "            block_X, block_y = block\n",
    "            \n",
    "            block_X = block_X.to(device)\n",
    "            block_y = block_y.to(device)\n",
    "            output_valid = model(block_X)\n",
    "            valid_loss = loss_fn(output_valid, block_y)\n",
    "            total_valid_loss += valid_loss.data.item()\n",
    "        BCELoss = total_loss/len(train_loader)\n",
    "        BCELoss_list.append(BCELoss)\n",
    "        valid_BCELoss = total_valid_loss/len(valid_loader)\n",
    "        valid_BCELoss_list.append(valid_BCELoss)\n",
    "        if valid_BCELoss < best_valid_BCELoss: \n",
    "            #dummy_input = torch.tensor(block_X).to(device).long()\n",
    "            #torch.onnx.export(model, dummy_input, f\"models/best_model.onnx\")\n",
    "            best_valid_BCELoss = BCELoss\n",
    "        \n",
    "        desc = (f'Epoch: {epoch}, train_loss: {BCELoss}, valid_loss: {valid_BCELoss}')\n",
    "        print(desc)\n",
    "        to_append = [epoch, BCELoss, valid_BCELoss]\n",
    "        report_train_length = len(report_train)\n",
    "        report_train.loc[report_train_length] = to_append\n",
    "        #print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))\n",
    "        display.clear_output(wait=True)\n",
    "        if epoch == 1:\n",
    "            pl.plot(BCELoss_list, '-b', label=\"TrainError\")\n",
    "            pl.plot(valid_BCELoss_list, '-r', label=\"ValidationError\")\n",
    "            pl.legend(loc='upper right')\n",
    "        else:\n",
    "            pl.plot(BCELoss_list, '-b')\n",
    "            pl.plot(valid_BCELoss_list, '-r')\n",
    "        pl.xlim(1, epochs)\n",
    "        pl.ylim(0, 2)\n",
    "        display.display(pl.gcf())\n",
    "        time.sleep(1.0)\n",
    "    return report_train, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from train.model import LSTMClassifier\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "report_train, trained_model = train(model, train_sample_dl, valid_sample_dl, 5, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"enhanced\",\n",
    "                    py_version=\"py3\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    " hyperparameters={\n",
    "                        'epochs': 20,\n",
    "                        'hidden_dim': 200,\n",
    "     \"vocab_size\": new_vocab_size\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1,instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_pred = pd.concat([pd.DataFrame(test_X_tf_len), pd.DataFrame(test_tf_X)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
    "\n",
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in tqdm(split_array):\n",
    "        predictions = np.append(predictions, predictor.predict(array))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_X_pred.values)\n",
    "predictions = [round(num) for num in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyperparameter_tuner = HyperparameterTuner(estimator = estimator, # The estimator object to use as the basis for the training jobs.\n",
    "                                               objective_metric_name = 'validation:loss',\n",
    "                                               metric_definitions = [{\"Name\": \"validation:loss\",\"Regex\": \"BCELoss: (.*?);\"}],# The metric used to compare trained models.\n",
    "                                               objective_type = 'Minimize', # Whether we wish to minimize or maximize the metric.\n",
    "                                               max_jobs = 4, # The total number of models to train\n",
    "                                               max_parallel_jobs = 1, # The number of models to train in parallel\n",
    "                                               hyperparameter_ranges = {\n",
    "                                                    'epochs': IntegerParameter(10,11),\n",
    "                                                    'hidden_dim': IntegerParameter(200,201),\n",
    "                                                    #'embedding_dim': IntegerParameter(32,40)\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyperparameter_tuner.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyperparameter_tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = xgb_hyperparameter_tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = sagemaker.estimator.Estimator.attach(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = best_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "model = PyTorchModel(model_data=best_estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     py_version=\"py3\",\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
    "\n",
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in tqdm(split_array):\n",
    "        predictions = np.append(predictions, predictor.predict(array))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_reviews(data_dir='data/aclImdb', stop=5000):\n",
    "    \n",
    "    results = []\n",
    "    ground = []\n",
    "    \n",
    "    # We make sure to test both positive and negative reviews    \n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        \n",
    "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n",
    "        files = glob.glob(path)\n",
    "        \n",
    "        files_read = 0\n",
    "        \n",
    "        print('Starting ', sentiment, ' files')\n",
    "        \n",
    "        # Iterate through the files and send them to the predictor\n",
    "        for f in tqdm(files):\n",
    "            with open(f) as review:\n",
    "                # First, we store the ground truth (was the review positive or negative)\n",
    "                try:\n",
    "                    # Read in the review and convert to 'utf-8' for transmission via HTTP\n",
    "                    review_input = review.read().encode('utf-8')\n",
    "                    # Send the review to the predictor and store the results\n",
    "                    results.append(float(predictor.predict(review_input)))\n",
    "                    if sentiment == 'pos':\n",
    "                        ground.append(1)\n",
    "                    else:\n",
    "                        ground.append(0)\n",
    "                except:\n",
    "                    pass\n",
    "                #print(results)\n",
    "            # Sending reviews to our endpoint one at a time takes a while so we\n",
    "            # only send a small number of reviews\n",
    "            files_read += 1\n",
    "            if files_read == stop:\n",
    "                break\n",
    "            \n",
    "    return ground, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground, results = test_reviews()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for idx in tqdm(range(n_test_sample)):\n",
    "    pr_data_X = test_tf_X[idx]\n",
    "    pr_data_len = test_X_tf_len[idx]\n",
    "    pr_data_pack = np.hstack((pr_data_len, pr_data_X))\n",
    "    pr_data_pack = pr_data_pack.reshape(1, -1)\n",
    "    pr_data = torch.from_numpy(pr_data_pack)\n",
    "    pr_data = pr_data.to(device)\n",
    "    model.eval()\n",
    "    result = model(pr_data).detach().numpy()\n",
    "    predicted_values.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "roc_auc = roc_auc_score(ground, results)\n",
    "fpr, tpr, _ = roc_curve(ground, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "accuracy_score(ground, np.rint(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(ground, np.rint(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
